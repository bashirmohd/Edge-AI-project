{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VJ7lfTYz0qu",
        "outputId": "6b0e6b47-aca1-4cff-ff36-f561366ec1c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "!pip -q install datasets sentencepiece \n",
        "!pip -q install bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Dolly 2.0\n"
      ],
      "metadata": {
        "id": "Wwn1vLOW-VQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdVSk5iZ1DVB",
        "outputId": "81ec9c6d-1607-4887-fd02-f556c6c01209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  1 00:33:33 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "generate_text = pipeline(model=\"databricks/dolly-v2-7b\", \n",
        "                         torch_dtype=torch.bfloat16, \n",
        "                         trust_remote_code=True, \n",
        "                         device_map=\"auto\"\n",
        "                         )\n"
      ],
      "metadata": {
        "id": "_KXYRwkFFS2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(generate_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d55QO8cJUHA",
        "outputId": "a4d36149-7007-4049-f8af-5ecfe7a55551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers_modules.databricks.dolly-v2-7b.97611f20f95e1d8c1e914b85da55cc3937c31192.instruct_pipeline.InstructionTextGenerationPipeline"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  GenerationConfig, pipeline\n",
        "import torch\n",
        "import textwrap"
      ],
      "metadata": {
        "id": "MK_96JPU0fgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJSkqjrYva2a",
        "outputId": "70694f3a-3e36-46b2-9df9-488922c89a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon May  1 00:35:43 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P0    29W /  70W |  14087MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "odWWkO_81HIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run it as a HF model"
      ],
      "metadata": {
        "id": "D_MBqSkZ1iQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text('What are the difference between Llamas Alpacas and Koalas?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1WqdUimN5H-",
        "outputId": "8ea4ad24-eee0-4187-f085-03a937c9d53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Llamas are native to South America and are smaller relatives of the alpaca family. They are usually around 6-8 feet tall and have long, wooly coats that can be green, gray, brown, or blond. Llamas are usually around 7-9 pounds.\\n\\nKoalas are found in Australia and are also a marsupial. They have a round, blob-shaped face, and distinctive hairy cheeks. They have large, white fluffy tails and short, round bodies that are slightly bent at the waist. They can reach weights of up to 20 pounds.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('What are the difference between Llamas, Alpacas and Koalas?')\n",
        "print(output[0][\"generated_text\"])\n"
      ],
      "metadata": {
        "id": "xGGmYmTC31om",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb11a02-ca8b-4912-8d62-12b0b156c5a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llamas, Alpacas and Koalas are all members of the species Andropogoneae. They are all ruminants, meaning they have a rumen, which is a fermentation chamber located in their lower abdomen. Koalas are the only Australian species, the others are from South America. Alpacas are the tallest of the group at 12 feet, Llamas are the smallest at 5 feet. The two larger species eat only the tough cud, the fructooligosaccharides and cellulose that most plants cannot access. Alpacas only have two teeth, where the other species have four. Koalas have a third, small finger like structure on their tail used for grooming.\n",
            "CPU times: user 4min 19s, sys: 335 ms, total: 4min 19s\n",
            "Wall time: 1min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('Write a short note to Sam Altman giving reasons to open source GPT-4')\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R846mNA-EPJl",
        "outputId": "0aab3547-6f4e-4d63-9e17-389129aa8049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"Hi Sam,\\n\\nAs a long-time user of your company's A.I., I've always been interested in its evolution and technology. Seeing that GPT-4 is just a bigger version of your earlier models, it makes sense to open source GPT-4 so that the wider community can benefit from your research and keep pace with the latest developments in A.I. technology. \\n\\nGPT-4 is a powerful model that could revolutionise text understanding and customer service, and could even have an impact on industries outside of software like healthcare and financial services.\\n\\nThank you for considering my request. I hope you will agree.\\n\\nWith gratitude,\\nYour loyal customer, \\n\\nWill\"}]\n",
            "CPU times: user 4min 17s, sys: 312 ms, total: 4min 17s\n",
            "Wall time: 1min 4s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('What is the capital of England? \\n')\n",
        "\n",
        "print(output[2][\"generated_text\"])"
      ],
      "metadata": {
        "id": "etXw2n6mD_4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('Write a story about a Koala playing pool and beating all the camelids.')\n",
        "\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pE2hSvhsEic",
        "outputId": "79d77624-b380-40ae-8ccb-1fabab5607fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time there was a Koala named Camelus Camelus who loved to play pool and beat all the camelids. One day a unicorn walked into his favorite pool hall and asked if he could play. Camelus let the unicorn play for a while and watched as she made some tough shots. She then knocked the balls into the cushion with a smack and said \"that's hard, that's hard\". Camelus was impressed, and told the unicorn that she could play if she wanted to. The unicorn said \"Sure!\". So Camelusный Koala friend played pool for a while, and then had to go to the bathroom, so she left. The unicorn then asked Camelus to play again, so he could see how good she really was. Camelus let the unicorn play another game, and he was impressed again, but this time said \"that's easy, that's easy\". Camelus said \"That's easy, just look at what you're doing. You're moving all those balls with your horn, that's hard! Now you're taking a drink, that's hard! Now you're making a shot, that's hard! Now you're putting your ball back\n",
            "CPU times: user 7min, sys: 570 ms, total: 7min 1s\n",
            "Wall time: 1min 45s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time \n",
        "output = generate_text('As an AI do you like the Simpsons? What dow you know about Homer?')\n",
        "\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQTgOX168NeK",
        "outputId": "c7d1fe3e-b38f-4510-bad2-e6b910eac340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As an AI I do not like the Simpsons. I do not know much about Homer.\n",
            "CPU times: user 1min 16s, sys: 95.6 ms, total: 1min 16s\n",
            "Wall time: 19.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Nil30lVDLHr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}